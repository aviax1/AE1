{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of myproj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObBqdi5ixstGyvG7KtmkIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aviax1/AE1/blob/master/index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtaGi-L_cSaq",
        "colab_type": "text"
      },
      "source": [
        "**dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIO3PVG_grNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used snniped from https://github.com/L1aoXingyu/pytorch-beginner/\n",
        "!pip install wandb\n",
        "import torch\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from multiprocessing import Process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J759cYYvgp4G"
      },
      "source": [
        "**initial**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGJFvD0b_rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(xtrain,ytrain), (xtest,ytest) = mnist.load_data()\n",
        "num_epochs=1000        #\n",
        "batch_size = 64        #\n",
        "image_size=784         #\n",
        "hidden_size=64         #\n",
        "lv_size = 64           # Latent Variable \n",
        "learning_rate=1e-4*1.3 #\n",
        "cret = nn.MSELoss()    # criterion"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CyNraKighz1W"
      },
      "source": [
        "**build model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUIqLQNnh3-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(image_size, hidden_size),   #nn.ReLU(True), nn.Linear(image_size, hidden_size),nn.ReLU(True), nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(True), nn.Linear(hidden_size, hidden_size), nn.ReLU(True), nn.Linear(hidden_size, lv_size))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(lv_size, hidden_size),nn.ReLU(True),nn.Linear(hidden_size, hidden_size),nn.ReLU(True), nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(True),nn.Linear(hidden_size, hidden_size),nn.ReLU(True), nn.Linear(hidden_size, image_size), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgQmvLNFnEAs"
      },
      "source": [
        "**model setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGfS3vTnZb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = autoencoder()\n",
        "tmodel=autoencoder()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5*0.2)\n",
        "\n",
        "class DigitDataSet(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "      self.dataset = dataset\n",
        "      self.transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      if torch.is_tensor(idx):\n",
        "          idx = idx.tolist()\n",
        "      return self.transform( self.dataset[idx,:,:])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WLnotTGWCoiu"
      },
      "source": [
        "**classsifcation by train models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "650-wSQ1CtXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testmodel(): \n",
        "  nn=len(ytest)\n",
        "  dataloader = DataLoader(DigitDataSet(xtest), batch_size=nn,shuffle=0 , num_workers=4)\n",
        "  diff = np.zeros( (nn,10),dtype=np.float32 )\n",
        "  for i in range(10):\n",
        "    for data in dataloader:\n",
        "      input_imgs = data\n",
        "      imgs = Variable(input_imgs.view(input_imgs.size(0), -1))\n",
        "      model_name='./ae_'+str(i)+'.pth'\n",
        "      tmodel.load_state_dict(torch.load(model_name))\n",
        "      tmodel.eval()\n",
        "      output_imgs = tmodel(imgs)\n",
        "      for i2 in range(len( output_imgs[:,0])):\n",
        "        im_pred=output_imgs.detach().numpy()[i2,:]\n",
        "        im_org=imgs.numpy()[i2,:]\n",
        "        difmat=np.abs(im_pred.reshape(28,28)-im_org.reshape(28,28))\n",
        "        diff[i2,i]=np.sum( np.sum( difmat ))\n",
        "  min_index=np.argmin(diff, axis=1)\n",
        "  seccess =  min_index == ytest\n",
        "  counts, bins = np.histogram(ytest[ min_index != ytest ])\n",
        "  plt.hist(bins[:-1], bins, weights=counts)\n",
        "  plt.title(\"error by digit\")\n",
        "  plt.show()\n",
        "  print(str(int(10000*np.sum(seccess))/(nn*100)) + \"% seccess rate \")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVPozo4nFAaJ",
        "colab_type": "text"
      },
      "source": [
        "**train model by digit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roL_TvrVC7g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(digit,model):\n",
        "  model_name='./ae_'+str(digit)+'.pth'\n",
        "  torch.save(model.state_dict(),model_name )\n",
        "  wandb.save(model_name)\n",
        "  print(\"save model \"+ model_name)\n",
        "\n",
        "def train_by_digit(by_digit,model):\n",
        "  wandb.init()\n",
        "  print(\"*****\\nstart traning Model for digit \" +str(by_digit) +\"\\n\")\n",
        "  dataloader = DataLoader(DigitDataSet(xtrain[ytrain==by_digit]), batch_size=batch_size,shuffle=True, num_workers=16)\n",
        "  for epoch in range(num_epochs):\n",
        "    run=  epoch%25==0\n",
        "    run2= epoch%125==0 and epoch >0\n",
        "    for data in dataloader:\n",
        "      imgs = Variable(data.view(data.size(0), -1))\n",
        "      output_imgs = model(imgs)\n",
        "      loss = cret(output_imgs, imgs)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if run:\n",
        "        run=0\n",
        "        im=data[0,0,:,:].reshape(28,28)\n",
        "        pred=model(imgs).detach().numpy()[0,:].reshape(28,28)\n",
        "        wandb.log({\"img\": [wandb.Image(pred, caption=\"preidciton\"),wandb.Image(im, caption=\"original\")]})\n",
        "      if run2:\n",
        "        run2=0\n",
        "        save_model(by_digit,model)\n",
        "        testmodel()\n",
        "        \n",
        "    print('epoch [{}/{}], loss:{:.4f}' .format(epoch + 1, num_epochs, loss.data))\n",
        "    wandb.log({\"loss\": loss.data})\n",
        "  save_model(by_digit,model)\n",
        "  print(\"\\nfinish traning Model Number \" +str(by_digit) +\"\\n*****\\n\")\n",
        "\n",
        "for by_digit in range(3,4):\n",
        "  train_by_digit(by_digit,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di24IvelKNyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "f4a0ce15-b291-48b6-f56f-322e7ae94b2f"
      },
      "source": [
        "testmodel()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "97.43% seccess rate \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}