{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of myproj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNm0n5/J43SyuMBC54fnpnn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aviax1/AE1/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnlUAkduSqay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtaGi-L_cSaq",
        "colab_type": "text"
      },
      "source": [
        "**dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIO3PVG_grNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used snniped from https://github.com/L1aoXingyu/pytorch-beginner/\n",
        "import torch\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J759cYYvgp4G"
      },
      "source": [
        "**initial**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGJFvD0b_rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(xtrain,ytrain), (xtest,ytest) = mnist.load_data()\n",
        "num_epochs=200      #\n",
        "batch_size = 32     #\n",
        "image_size=784      #\n",
        "hidden_size=256     #\n",
        "lv_size = 64        # Latent Variable \n",
        "learning_rate=1e-5  #\n",
        "cret = nn.MSELoss() # criterion"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CyNraKighz1W"
      },
      "source": [
        "**build model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUIqLQNnh3-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(image_size, image_size),nn.ReLU(True), nn.Linear(image_size, hidden_size),nn.ReLU(True), nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(True), nn.Linear(hidden_size, hidden_size), nn.ReLU(True), nn.Linear(hidden_size, lv_size))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(lv_size, hidden_size),nn.ReLU(True),nn.Linear(hidden_size, hidden_size),nn.ReLU(True), nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(True),nn.Linear(hidden_size, hidden_size),nn.ReLU(True), nn.Linear(hidden_size, image_size), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgQmvLNFnEAs"
      },
      "source": [
        "**model setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGfS3vTnZb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = autoencoder()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "class DigitDataSet(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "      self.dataset = dataset\n",
        "      self.transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      if torch.is_tensor(idx):\n",
        "          idx = idx.tolist()\n",
        "      return self.transform( self.dataset[idx,:,:])\n",
        "\n",
        "def images_row(dis_images,title,add_to_index=0, images_in_row=5):\n",
        "  if( len(np.shape(dis_images)) == 2):\n",
        "    dis_images=dis_images[0:images_in_row,:]\n",
        "  else:\n",
        "    dis_images=dis_images[0:images_in_row,0,:,:]\n",
        "  for i in range(len(dis_images)):\n",
        "    ax = plt.subplot(30, images_in_row, i+add_to_index + 1)\n",
        "    plt.imshow(dis_images[i].reshape(28, 28))\n",
        "    plt.title( title)\n",
        "    plt.gray()\n",
        "\n",
        "def visual_epoch(epoch_num,model,dataloader):\n",
        "  for data in dataloader:\n",
        "      input_imgs = data\n",
        "      imgs = Variable(input_imgs.view(input_imgs.size(0), -1))\n",
        "      output_imgs = model(imgs)\n",
        "      images_row(input_imgs,\"org \",5*epoch_num)\n",
        "      images_row(output_imgs.detach().numpy(),\"rec \",5*(epoch_num+1))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVPozo4nFAaJ",
        "colab_type": "text"
      },
      "source": [
        "**train model by digit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roL_TvrVC7g9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6a27672-5977-4ef6-e812-54ca883fe0d4"
      },
      "source": [
        "def train_by_digit(by_digit,model):\n",
        "  wandb.init()\n",
        "  wandb.watch(model)\n",
        "  print(\"*****\\nstart traning Model for digit \" +str(by_digit) +\"\\n\")\n",
        "  dataloader = DataLoader(DigitDataSet(xtrain[ytrain==by_digit]), batch_size=batch_size,shuffle=True, num_workers=4)\n",
        "  visual_counter=0\n",
        "  for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "      input_imgs = data\n",
        "      imgs = Variable(input_imgs.view(input_imgs.size(0), -1))\n",
        "      output_imgs = model(imgs)\n",
        "      loss = cret(output_imgs, imgs)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    #if epoch%3==0 :\n",
        "    #  visual_epoch(visual_counter,model,dataloader)\n",
        "    print('epoch [{}/{}], loss:{:.4f}' .format(epoch + 1, num_epochs, loss.data))\n",
        "    wandb.log({\"loss\": loss.data})\n",
        "\n",
        "  torch.save(model.state_dict(), './ae_'+str(by_digit)+'.pth')\n",
        "  print(\"\\nfinish traning Model Number \" +str(by_digit) +\"\\n\")\n",
        "  print(\"*****\\n\")\n",
        "\n",
        "for by_digit in range(2,10):\n",
        "  train_by_digit(by_digit,model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/aviax1/uncategorized\" target=\"_blank\">https://app.wandb.ai/aviax1/uncategorized</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/aviax1/uncategorized/runs/3h238mf7\" target=\"_blank\">https://app.wandb.ai/aviax1/uncategorized/runs/3h238mf7</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "*****\n",
            "start traning Model for digit 2\n",
            "\n",
            "epoch [1/200], loss:0.6374\n",
            "epoch [2/200], loss:0.2947\n",
            "epoch [3/200], loss:0.2306\n",
            "epoch [4/200], loss:0.2532\n",
            "epoch [5/200], loss:0.2829\n",
            "epoch [6/200], loss:0.2468\n",
            "epoch [7/200], loss:0.2553\n",
            "epoch [8/200], loss:0.2294\n",
            "epoch [9/200], loss:0.3050\n",
            "epoch [10/200], loss:0.2376\n",
            "epoch [11/200], loss:0.2538\n",
            "epoch [12/200], loss:0.2469\n",
            "epoch [13/200], loss:0.2064\n",
            "epoch [14/200], loss:0.2148\n",
            "epoch [15/200], loss:0.2388\n",
            "epoch [16/200], loss:0.1944\n",
            "epoch [17/200], loss:0.1964\n",
            "epoch [18/200], loss:0.1877\n",
            "epoch [19/200], loss:0.2218\n",
            "epoch [20/200], loss:0.1647\n",
            "epoch [21/200], loss:0.2103\n",
            "epoch [22/200], loss:0.2131\n",
            "epoch [23/200], loss:0.1878\n",
            "epoch [24/200], loss:0.1767\n",
            "epoch [25/200], loss:0.1885\n",
            "epoch [26/200], loss:0.2094\n",
            "epoch [27/200], loss:0.1497\n",
            "epoch [28/200], loss:0.1699\n",
            "epoch [29/200], loss:0.1689\n",
            "epoch [30/200], loss:0.1805\n",
            "epoch [31/200], loss:0.2260\n",
            "epoch [32/200], loss:0.1486\n",
            "epoch [33/200], loss:0.1922\n",
            "epoch [34/200], loss:0.1500\n",
            "epoch [35/200], loss:0.1842\n",
            "epoch [36/200], loss:0.1479\n",
            "epoch [37/200], loss:0.1746\n",
            "epoch [38/200], loss:0.1563\n",
            "epoch [39/200], loss:0.1382\n",
            "epoch [40/200], loss:0.1559\n",
            "epoch [41/200], loss:0.1362\n",
            "epoch [42/200], loss:0.1740\n",
            "epoch [43/200], loss:0.1484\n",
            "epoch [44/200], loss:0.1815\n",
            "epoch [45/200], loss:0.1245\n",
            "epoch [46/200], loss:0.1193\n",
            "epoch [47/200], loss:0.1532\n",
            "epoch [48/200], loss:0.1425\n",
            "epoch [49/200], loss:0.1315\n",
            "epoch [50/200], loss:0.1334\n",
            "epoch [51/200], loss:0.1405\n",
            "epoch [52/200], loss:0.1392\n",
            "epoch [53/200], loss:0.1242\n",
            "epoch [54/200], loss:0.1381\n",
            "epoch [55/200], loss:0.1262\n",
            "epoch [56/200], loss:0.1414\n",
            "epoch [57/200], loss:0.1085\n",
            "epoch [58/200], loss:0.0904\n",
            "epoch [59/200], loss:0.1328\n",
            "epoch [60/200], loss:0.1350\n",
            "epoch [61/200], loss:0.1305\n",
            "epoch [62/200], loss:0.1240\n",
            "epoch [63/200], loss:0.1212\n",
            "epoch [64/200], loss:0.1040\n",
            "epoch [65/200], loss:0.1053\n",
            "epoch [66/200], loss:0.1036\n",
            "epoch [67/200], loss:0.1380\n",
            "epoch [68/200], loss:0.1198\n",
            "epoch [69/200], loss:0.1288\n",
            "epoch [70/200], loss:0.1638\n",
            "epoch [71/200], loss:0.0963\n",
            "epoch [72/200], loss:0.1309\n",
            "epoch [73/200], loss:0.1021\n",
            "epoch [74/200], loss:0.1246\n",
            "epoch [75/200], loss:0.1092\n",
            "epoch [76/200], loss:0.1033\n",
            "epoch [77/200], loss:0.1097\n",
            "epoch [78/200], loss:0.1505\n",
            "epoch [79/200], loss:0.1111\n",
            "epoch [80/200], loss:0.1047\n",
            "epoch [81/200], loss:0.1191\n",
            "epoch [82/200], loss:0.1191\n",
            "epoch [83/200], loss:0.1144\n",
            "epoch [84/200], loss:0.1046\n",
            "epoch [85/200], loss:0.1135\n",
            "epoch [86/200], loss:0.1097\n",
            "epoch [87/200], loss:0.1264\n",
            "epoch [88/200], loss:0.1102\n",
            "epoch [89/200], loss:0.1332\n",
            "epoch [90/200], loss:0.0915\n",
            "epoch [91/200], loss:0.1368\n",
            "epoch [92/200], loss:0.0925\n",
            "epoch [93/200], loss:0.1180\n",
            "epoch [94/200], loss:0.1152\n",
            "epoch [95/200], loss:0.1178\n",
            "epoch [96/200], loss:0.1064\n",
            "epoch [97/200], loss:0.0992\n",
            "epoch [98/200], loss:0.0967\n",
            "epoch [99/200], loss:0.1358\n",
            "epoch [100/200], loss:0.1061\n",
            "epoch [101/200], loss:0.1064\n",
            "epoch [102/200], loss:0.1527\n",
            "epoch [103/200], loss:0.1140\n",
            "epoch [104/200], loss:0.1132\n",
            "epoch [105/200], loss:0.1320\n",
            "epoch [106/200], loss:0.1067\n",
            "epoch [107/200], loss:0.1174\n",
            "epoch [108/200], loss:0.0937\n",
            "epoch [109/200], loss:0.0947\n",
            "epoch [110/200], loss:0.1224\n",
            "epoch [111/200], loss:0.0920\n",
            "epoch [112/200], loss:0.1108\n",
            "epoch [113/200], loss:0.0922\n",
            "epoch [114/200], loss:0.0969\n",
            "epoch [115/200], loss:0.1267\n",
            "epoch [116/200], loss:0.0927\n",
            "epoch [117/200], loss:0.0957\n",
            "epoch [118/200], loss:0.1217\n",
            "epoch [119/200], loss:0.0922\n",
            "epoch [120/200], loss:0.0912\n",
            "epoch [121/200], loss:0.0885\n",
            "epoch [122/200], loss:0.1138\n",
            "epoch [123/200], loss:0.1031\n",
            "epoch [124/200], loss:0.0990\n",
            "epoch [125/200], loss:0.0862\n",
            "epoch [126/200], loss:0.1143\n",
            "epoch [127/200], loss:0.0932\n",
            "epoch [128/200], loss:0.1013\n",
            "epoch [129/200], loss:0.0999\n",
            "epoch [130/200], loss:0.1026\n",
            "epoch [131/200], loss:0.0811\n",
            "epoch [132/200], loss:0.0840\n",
            "epoch [133/200], loss:0.0991\n",
            "epoch [134/200], loss:0.1010\n",
            "epoch [135/200], loss:0.1057\n",
            "epoch [136/200], loss:0.0900\n",
            "epoch [137/200], loss:0.0939\n",
            "epoch [138/200], loss:0.0829\n",
            "epoch [139/200], loss:0.1156\n",
            "epoch [140/200], loss:0.1185\n",
            "epoch [141/200], loss:0.0978\n",
            "epoch [142/200], loss:0.0912\n",
            "epoch [143/200], loss:0.0833\n",
            "epoch [144/200], loss:0.1037\n",
            "epoch [145/200], loss:0.0947\n",
            "epoch [146/200], loss:0.0736\n",
            "epoch [147/200], loss:0.1039\n",
            "epoch [148/200], loss:0.1008\n",
            "epoch [149/200], loss:0.0808\n",
            "epoch [150/200], loss:0.0910\n",
            "epoch [151/200], loss:0.0996\n",
            "epoch [152/200], loss:0.0769\n",
            "epoch [153/200], loss:0.1005\n",
            "epoch [154/200], loss:0.1097\n",
            "epoch [155/200], loss:0.1022\n",
            "epoch [156/200], loss:0.0885\n",
            "epoch [157/200], loss:0.1081\n",
            "epoch [158/200], loss:0.0894\n",
            "epoch [159/200], loss:0.0930\n",
            "epoch [160/200], loss:0.0913\n",
            "epoch [161/200], loss:0.0813\n",
            "epoch [162/200], loss:0.0936\n",
            "epoch [163/200], loss:0.1144\n",
            "epoch [164/200], loss:0.0981\n",
            "epoch [165/200], loss:0.0875\n",
            "epoch [166/200], loss:0.0901\n",
            "epoch [167/200], loss:0.1106\n",
            "epoch [168/200], loss:0.0764\n",
            "epoch [169/200], loss:0.0910\n",
            "epoch [170/200], loss:0.0831\n",
            "epoch [171/200], loss:0.0815\n",
            "epoch [172/200], loss:0.0817\n",
            "epoch [173/200], loss:0.0984\n",
            "epoch [174/200], loss:0.0712\n",
            "epoch [175/200], loss:0.0954\n",
            "epoch [176/200], loss:0.0905\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}